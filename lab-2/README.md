### 1. Власні значення і власні вектори матриці

#### Що таке власне значення і власний вектор матриці?

Власне значення (eigenvalue) і власний вектор (eigenvector) — це важливі поняття в лінійній алгебрі, які використовуються для аналізу лінійних перетворень.

**Визначення:**
- **Власний вектор** матриці $A$ — це ненульовий вектор $\mathbf{v}$, для якого виконується рівність $A\mathbf{v} = \lambda \mathbf{v}$, де $\lambda$ — це скаляр, який називається власним значенням, відповідним цьому власному вектору.
- **Власне значення** $\lambda$ матриці $A$ — це число, при якому рівняння $A\mathbf{v} = \lambda \mathbf{v}$ має ненульове рішення для вектора $\mathbf{v}$.

#### Як вони обчислюються?

Щоб знайти власні значення і власні вектори матриці $A$, виконуємо такі кроки:

1. **Знаходимо власні значення:** 
   - Розв'язуємо характеристичне рівняння $\det(A - \lambda I) = 0$, де $I$ — одинична матриця такого ж розміру, як і $A$.
   - Значення $\lambda$, які задовольняють це рівняння, є власними значеннями матриці $A$.

2. **Знаходимо власні вектори:**
   - Для кожного власного значення $\lambda$ розв'язуємо систему рівнянь $(A - \lambda I)\mathbf{v} = 0$.
   - Вектори $\mathbf{v}$, які є розв'язками цієї системи, є власними векторами, відповідними власному значенню $\lambda$.

**Приклад:**

Розглянемо матрицю $A$:

$ A = \begin{pmatrix} 4 & 1 \\ 2 & 3 \end{pmatrix} $

1. **Знаходимо власні значення:**

   $
   \det(A - \lambda I) = \det \begin{pmatrix} 4 - \lambda & 1 \\ 2 & 3 - \lambda \end{pmatrix} = (4 - \lambda)(3 - \lambda) - 2 = \lambda^2 - 7\lambda + 10 = 0
   $

   Розв'язуємо характеристичне рівняння:

   $
   \lambda^2 - 7\lambda + 10 = 0 \implies \lambda = 5, \lambda = 2
   $

2. **Знаходимо власні вектори:**

   Для $\lambda = 5$:

   $
   (A - 5I)\mathbf{v} = \begin{pmatrix} -1 & 1 \\ 2 & -2 \end{pmatrix} \mathbf{v} = 0
   $

   Розв'язуємо цю систему:

   $
   \mathbf{v}_1 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}
   $

   Для $\lambda = 2$:

   $
   (A - 2I)\mathbf{v} = \begin{pmatrix} 2 & 1 \\ 2 & 1 \end{pmatrix} \mathbf{v} = 0
   $

   Розв'язуємо цю систему:

   $
   \mathbf{v}_2 = \begin{pmatrix} 1 \\ -2 \end{pmatrix}
   $

Таким чином, власні значення матриці $A$ — 5 і 2, відповідні власні вектори — $\begin{pmatrix} 1 \\ 1 \end{pmatrix}$ і $\begin{pmatrix} 1 \\ -2 \end{pmatrix}$.

### 2. Властивості власних векторів симетричних матриць

Симетричні матриці мають кілька важливих властивостей, що стосуються власних векторів:

1. **Власні значення симетричної матриці є дійсними.**
2. **Власні вектори, що відповідають різним власним значенням, є ортогональними.** Це означає, що якщо $\mathbf{v}_1$ і $\mathbf{v}_2$ є власними векторами симетричної матриці з різними власними значеннями, то $\mathbf{v}_1 \cdot \mathbf{v}_2 = 0$.
3. **Симетричну матрицю можна діагоналізувати ортогональною матрицею.** Це означає, що симетричну матрицю $A$ можна подати у вигляді $A = PDP^T$, де $P$ — ортогональна матриця (матриця, стовпці якої є власними векторами), а $D$ — діагональна матриця власних значень.

### 3. Недоліки використання PCA і стратегії їх подолання

Principal Component Analysis (PCA) — це метод зниження розмірності даних, але він має кілька недоліків:

1. **Лінійність PCA:**
   - Недолік: PCA є лінійним методом і не може ефективно обробляти нелінійні взаємозв'язки в даних.
   - Стратегія подолання: Використовувати нелінійні варіанти PCA, такі як Kernel PCA, які дозволяють враховувати нелінійні взаємозв'язки.

2. **Втрата інтерпретованості:**
   - Недолік: Знижуючи розмірність, PCA може втратити значущу інтерпретацію оригінальних ознак.
   - Стратегія подолання: Зберігати оригінальні ознаки поряд з новими компонентами або використовувати методи, що забезпечують кращу інтерпретацію, такі як Sparse PCA.

3. **Чутливість до масштабування:**
   - Недолік: PCA чутливий до масштабування даних; ознаки з більшими величинами можуть домінувати над ознаками з меншими величинами.
   - Стратегія подолання: Масштабувати дані перед застосуванням PCA (наприклад, використовувати StandardScaler для стандартизації ознак).

4. **Чутливість до шуму:**
   - Недолік: PCA може бути чутливим до шуму в даних, що може вплинути на виділення головних компонентів.
   - Стратегія подолання: Використовувати методи попередньої обробки даних для видалення шуму або застосовувати робастні варіанти PCA, такі як Robust PCA.

### 4. Переваги діагоналізації матриці в криптографії

Діагоналізація матриці має кілька переваг у криптографії:

1. **Прискорення обчислень:**
   - При діагоналізації матриці $A$ у вигляді $A = PDP^{-1}$, де $P$ — матриця власних векторів, а $D$ — діагональна матриця власних значень, обчислення стають більш ефективними. Замість множення на загальну матрицю $A$, можна виконувати множення на діагональну матрицю $D$, що є простішим і швидшим.

2. **Шифрування та дешифрування:**
   - У криптографії діагоналізація матриць може використовуватися для шифрування і дешифрування повідомлень. Застосування діагональної матриці $D$ до вектора дозволяє здійснювати шифрування, а застосування оберненої матриці $P^{-1}$ — дешифрування.

**Приклад:**

Припустимо, що ми маємо повідомлення, представлене вектором $\mathbf{m}$, і матрицю ключа $A$, яку можна діагоналізувати як $A = PDP^{-1}$:

1. **Шифрування:**
   - Шифроване повідомлення $\mathbf{c}$ обчислюється як $\mathbf{c} = A\mathbf{m}$.
   - Використовуючи діагоналізацію, це можна записати як $\mathbf{c} = PDP^{-1}\mathbf{m}$.

2. **

Дешифрування:**
   - Щоб розшифрувати повідомлення, знаходимо $\mathbf{m}$ з $\mathbf{c}$:
   - $\mathbf{m} = P^{-1}D^{-1}P\mathbf{c}$.

Таким чином, діагоналізація спрощує обчислення і робить процес шифрування/дешифрування більш ефективним.

